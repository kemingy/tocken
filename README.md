# Tocken

Tokenizer implemented in Rust.

This tokenizer is based on [Lucene's EnglishAnalyzer](https://github.com/apache/lucene/blob/525b963be076fe8c58dd1162f083b6a9911e4efd/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishAnalyzer.java#L37).
